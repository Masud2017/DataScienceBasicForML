{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import requests # for download the api responses\n",
    "import json # for work with json files\n",
    "from sqlalchemy import create_engine\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmpy():\n",
    "    for _ in range(100):\n",
    "        print(\"*\",end='')\n",
    "    print('')   \n",
    "    print(\"[+] New section uses of numpy :: For data science : \")\n",
    "    a = np.array([[1,2],[3,4]])\n",
    "    a2 = np.array([1,2,3,4,6,5,7,8,9],ndmin=2,dtype=complex)\n",
    "    print(a)\n",
    "    print(a2)\n",
    "\n",
    "    \n",
    "def pands():\n",
    "    # pandas handles data through Series, Data Frame and Panel\n",
    "    # Pandas Series (This is an one dimensional lebeled array)\n",
    "    # pandas.Series(data,index,dtype,copy)\n",
    "    data = np.array(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\n",
    "    s = pd.Series(data)\n",
    "    print(s)\n",
    "    # Data Frame ==> This is an two dimensional data structure\n",
    "    # pandas.DataFrame(data,index,column,dtype,copy)\n",
    "    data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}\n",
    "    df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])\n",
    "    print(df)\n",
    "    \n",
    "    # Pandas Panel\n",
    "    # A panel is a 3D container of data. The term Panel data is derived from econometrics and is partially responsible for the name pandas âˆ’ pan(el)-da(ta)-s.\n",
    "    # panas.Panel(data,items,major_axis,minor_axis,dtype,copy)\n",
    "    \n",
    "    df.plot.box(grid='True')\n",
    "\n",
    "def DataCleansing():\n",
    "    for _ in range(100):\n",
    "        print(\"*\",end='')\n",
    "    print('')\n",
    "    print(\"Data Cleansing\")\n",
    "    df = pd.DataFrame(np.random.randn(5,3),index=['a', 'c', 'e', 'f',\n",
    "'h'],columns=['one','two','three'])\n",
    "    print(df)\n",
    "    print('\\n\\n')\n",
    "    df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "    print(df)\n",
    "    print(df['one'].isnull())\n",
    "    \n",
    "    # cleaning/filling missing data\n",
    "    print(\"NaN replaced with '0':\")\n",
    "    print(df.fillna(0))\n",
    "    # We can fill NA forward and backward\n",
    "    print(df.fillna(method='pad')) # fill forward\n",
    "    print(df.fillna(method='bfill')) # fill backward\n",
    "    \n",
    "    # excluding or dropping the infected/NAN row\n",
    "    print(df.dropna())\n",
    "    \n",
    "    # replacing missing or Generic values\n",
    "     # Many times , we have to replace a generic value iwth some specifi\n",
    "     # value. We can achieve this by applying the replace method\n",
    "    dat = pd.DataFrame({'one':[10,20,30,40,50,2000],'two':[1000,0,30,40,50,60]},index=['A','B','C','D','E','F'])\n",
    "    print(dat.replace({1000:10,2000:60}))\n",
    "    \n",
    "def main():\n",
    "    url = \"https://www.googleapis.com/youtube/v3/channels?id=UCyQSAi4Xh5ZQKpMPLEUPSrA&key=AIzaSyBYzmjpQYOb15ueTx3-Y2QcgI8_21Xlhr0&part=snippet,statistics&fields=items(id,snippet,statistics)\"\n",
    "    print(\"Hello world\")\n",
    "    re = requests.get(url)\n",
    "   # reParsed = json.loads(re.text)\n",
    "    a = np.array([[1, 2], [3, 4]]) \n",
    "    data = {'Item1' : pd.DataFrame(np.random.randn(4, 3)), \n",
    "        'Item2' : pd.DataFrame(np.random.randn(4, 2))}\n",
    "    \n",
    "    df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f',\n",
    "    'h'],columns=['one', 'two', 'three'])\n",
    "    df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "    print(df)\n",
    "    print(type(df))\n",
    "    print(type(re))\n",
    "    #print(reParsed['items'][0]['id'])\n",
    "    \n",
    "def csvData():\n",
    "    # Processing csv data\n",
    "    for _ in range(50):\n",
    "        print(\"x\",end='')\n",
    "    print('')\n",
    "    print(\"[+] Processing the csv data\")\n",
    "    data = pd.read_csv('input.csv')\n",
    "    print(data)\n",
    "    # Slice the result for first 5 rows\n",
    "    print(data[0:5]['salary'])\n",
    "    # reading specific column using multiaxes indexing method .loc()\n",
    "    print(data.loc[:,['salary','name']])\n",
    "\n",
    "def jsonData():\n",
    "    # Processing JSON DATA\n",
    "    for _ in range(50):\n",
    "        print('x',end='')\n",
    "    print('')\n",
    "    print(\"[+] Processing the json data: \")\n",
    "    data = pd.read_json('data.json')\n",
    "    print(data)\n",
    "    # print (data.loc[[1,3,5],['salary','name']])\n",
    "    print(data.to_json(orient='records',lines=True))\n",
    "    \n",
    "def xlsxData():\n",
    "    # Prcessing XLSX data\n",
    "    for _ in range(50):\n",
    "        print('x',end='')\n",
    "    print('')\n",
    "    print(\"[+] Processing the xlsx data: \")\n",
    "    data = pd.read_excel('excelData.xlsx')\n",
    "    print(data)\n",
    "    \n",
    "def relDataBase():\n",
    "    # Relational Database\n",
    "    data = pd.read_csv('input.csv')\n",
    "    engine = create_engine('sqlite:///:memory:')\n",
    "    data.to_sql('data_table',engine)\n",
    "    res1 = pd.read_sql_query('SELECT * FROM data_table',engine)\n",
    "    print('Result1')\n",
    "    print(res1)\n",
    "    res2 = pd.read_sql_query('SELECT dept,sum(salary) FROM data_table group by dept', engine)\n",
    "    print('Result 2')\n",
    "    print(res2)\n",
    "    \n",
    "def bs():\n",
    "    # Reading data from html pages\n",
    "    res = requests.get(\"https://www.tutorialspoint.com/python_data_science/python_reading_html_pages.htm\")\n",
    "    html_doc = res.text\n",
    "    soup = BeautifulSoup(html_doc,'html.parser')\n",
    "    print(soup.title)\n",
    "    print(soup.title.string)\n",
    "    print(soup.a.string)\n",
    "    print(soup.p.string)\n",
    "    # Extracting all tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "[+] Processing the json data: \n",
      "   ID      Name  Salary   StartDate        Dept\n",
      "0   1      Rick  623.30    1/1/2012          IT\n",
      "1   2       Dan  515.20   9/23/2013  Operations\n",
      "2   3  Michelle  611.00  11/15/2014          IT\n",
      "3   4      Ryan  729.00   5/11/2014          HR\n",
      "4   5      Gary  843.25   3/27/2015     Finance\n",
      "5   6      Nina  578.00   5/21/2013          IT\n",
      "6   7     Simon  632.80   7/30/2013  Operations\n",
      "7   8      Guru  722.50   6/17/2014     Finance\n",
      "{\"ID\":1,\"Name\":\"Rick\",\"Salary\":623.3,\"StartDate\":\"1\\/1\\/2012\",\"Dept\":\"IT\"}\n",
      "{\"ID\":2,\"Name\":\"Dan\",\"Salary\":515.2,\"StartDate\":\"9\\/23\\/2013\",\"Dept\":\"Operations\"}\n",
      "{\"ID\":3,\"Name\":\"Michelle\",\"Salary\":611.0,\"StartDate\":\"11\\/15\\/2014\",\"Dept\":\"IT\"}\n",
      "{\"ID\":4,\"Name\":\"Ryan\",\"Salary\":729.0,\"StartDate\":\"5\\/11\\/2014\",\"Dept\":\"HR\"}\n",
      "{\"ID\":5,\"Name\":\"Gary\",\"Salary\":843.25,\"StartDate\":\"3\\/27\\/2015\",\"Dept\":\"Finance\"}\n",
      "{\"ID\":6,\"Name\":\"Nina\",\"Salary\":578.0,\"StartDate\":\"5\\/21\\/2013\",\"Dept\":\"IT\"}\n",
      "{\"ID\":7,\"Name\":\"Simon\",\"Salary\":632.8,\"StartDate\":\"7\\/30\\/2013\",\"Dept\":\"Operations\"}\n",
      "{\"ID\":8,\"Name\":\"Guru\",\"Salary\":722.5,\"StartDate\":\"6\\/17\\/2014\",\"Dept\":\"Finance\"}\n",
      "Result1\n",
      "   index  id    name  salary  start_date        dept\n",
      "0      0   1    Rick  623.30  2012-01-01          IT\n",
      "1      1   2     Dan  515.20  2013-09-23  Operations\n",
      "2      2   3   Tusar  611.00  2014-11-15          IT\n",
      "3      3   4    Ryan  729.00  2014-05-11          HR\n",
      "4      4   5    Gary  843.25  2015-03-27     Finance\n",
      "5      5   6   Rasmi  578.00  2013-05-21          IT\n",
      "6      6   7  Pranab  632.80  2013-07-30  Operations\n",
      "7      7   8    Guru  722.50  2014-06-17     Finance\n",
      "Result 2\n",
      "         dept  sum(salary)\n",
      "0     Finance      1565.75\n",
      "1          HR       729.00\n",
      "2          IT      1812.30\n",
      "3  Operations      1148.00\n",
      "<title>Python - Reading HTML Pages - Tutorialspoint</title>\n",
      "Python - Reading HTML Pages - Tutorialspoint\n",
      "None\n",
      "library known as beautifulsoup. Using this library, we can search for the values of html tags and get specific data like title of the page and the list of headers in the page.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #main() # executing the main method\n",
    "    #nmpy()\n",
    "    #pands()\n",
    "    #DataCleansing()\n",
    "    #csvData()\n",
    "    jsonData()\n",
    "    #xlsxData()\n",
    "    relDataBase()\n",
    "    bs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
